{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9692197,"sourceType":"datasetVersion","datasetId":5925625}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Neural Network From Scratch","metadata":{"id":"TqxT09VNxSPx"}},{"cell_type":"markdown","source":"This notebook implements a simple neural network from scratch using NumPy to classify handwritten digits from MNIST dataset.","metadata":{"id":"P15VehS4xbAM"}},{"cell_type":"code","source":"# importing necessary libraries for numerical operations, data manipulation and plotting images\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt        ","metadata":{"id":"IkKqz8aED1jS","execution":{"iopub.status.busy":"2024-10-23T18:06:04.304469Z","iopub.execute_input":"2024-10-23T18:06:04.304981Z","iopub.status.idle":"2024-10-23T18:06:04.785085Z","shell.execute_reply.started":"2024-10-23T18:06:04.304924Z","shell.execute_reply":"2024-10-23T18:06:04.783673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing the dataset from CSV file\ndata = pd.read_csv('/kaggle/input/mnist-dataset/mnist_train.csv')","metadata":{"id":"w6ZqqZasD8kN","execution":{"iopub.status.busy":"2024-10-23T18:06:04.788088Z","iopub.execute_input":"2024-10-23T18:06:04.788827Z","iopub.status.idle":"2024-10-23T18:06:10.806134Z","shell.execute_reply.started":"2024-10-23T18:06:04.788763Z","shell.execute_reply":"2024-10-23T18:06:10.804816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"id":"3vdrzzIrEkxM","outputId":"7c2e7ea4-e33c-4c04-d831-325409e9fe0f","execution":{"iopub.status.busy":"2024-10-23T18:06:10.807643Z","iopub.execute_input":"2024-10-23T18:06:10.808132Z","iopub.status.idle":"2024-10-23T18:06:10.854882Z","shell.execute_reply.started":"2024-10-23T18:06:10.808080Z","shell.execute_reply":"2024-10-23T18:06:10.853727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting the DataFrame to a NumPy array for easier manipulation\ndata = np.array(data)","metadata":{"id":"mvDcb48_En_Y","execution":{"iopub.status.busy":"2024-10-23T18:06:10.858086Z","iopub.execute_input":"2024-10-23T18:06:10.858655Z","iopub.status.idle":"2024-10-23T18:06:11.036236Z","shell.execute_reply.started":"2024-10-23T18:06:10.858594Z","shell.execute_reply":"2024-10-23T18:06:11.034884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the number of samples (m) and features (n)\nm, n = data.shape","metadata":{"id":"zvYFg_vNE8dQ","execution":{"iopub.status.busy":"2024-10-23T18:06:11.037616Z","iopub.execute_input":"2024-10-23T18:06:11.038003Z","iopub.status.idle":"2024-10-23T18:06:11.043358Z","shell.execute_reply.started":"2024-10-23T18:06:11.037962Z","shell.execute_reply":"2024-10-23T18:06:11.041874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shuffling the data to ensure randomness in training\nnp.random.shuffle(data)","metadata":{"id":"cb0XrlYFE-Yk","execution":{"iopub.status.busy":"2024-10-23T18:06:11.045093Z","iopub.execute_input":"2024-10-23T18:06:11.045633Z","iopub.status.idle":"2024-10-23T18:06:11.981059Z","shell.execute_reply.started":"2024-10-23T18:06:11.045574Z","shell.execute_reply":"2024-10-23T18:06:11.979671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the data into development and training sets\n# data_dev = data[0:1000].T         # Development set of 1000 samples\n# Y_dev = data_dev[0]               # Labels for the development set\n# X_dev = data_dev[1:n]             # Features for the development set\n# X_dev = X_dev / 255               # Normalizing the input data\n\ndata_train = data.T       # Training set\nY_train = data_train[0]           # Labels for the training set\nX_train = data_train[1:n]         # Features for the training set\nX_train = X_train / 255           # Normalizing the input data","metadata":{"id":"_DOeSEXQFC-0","execution":{"iopub.status.busy":"2024-10-23T18:06:11.982945Z","iopub.execute_input":"2024-10-23T18:06:11.983475Z","iopub.status.idle":"2024-10-23T18:06:12.182978Z","shell.execute_reply.started":"2024-10-23T18:06:11.983413Z","shell.execute_reply":"2024-10-23T18:06:12.181783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def initialize_parameters():\n    \"\"\"Initializes the parameters (weights and biases) for the neural network.\"\"\"\n    # Weight matrices are initialized randomly with values between -0.5 and 0.5\n    W1 = np.random.rand(64, 784) - 0.5  # Weights for the first layer (64 neurons, 784 inputs)\n    b1 = np.random.rand(64, 1) - 0.5    # Bias for the first layer\n    W2 = np.random.rand(32, 64) - 0.5    # Weights for the second layer (32 neurons, 64 inputs)\n    b2 = np.random.rand(32, 1) - 0.5    # Bias for the second layer\n    W3 = np.random.rand(10, 32) - 0.5    # Weights for the output layer (10 neurons, 32 inputs)\n    b3 = np.random.rand(10, 1) - 0.5    # Bias for the output layer\n    return W1, b1, W2, b2, W3, b3\n\n\n\ndef ReLU(Z):\n  \"\"\"Applies the ReLU activation function.\"\"\"\n  return np.maximum(Z, 0)\n\ndef softmax(Z):\n  \"\"\"Applies the softmax activation function.\"\"\"\n  A = np.exp(Z) / sum(np.exp(Z))\n  return A\n\n\ndef forward_prop(W1, b1, W2, b2, W3, b3, X):\n    \"\"\"Performs forward propagation through the neural network.\"\"\"\n    Z1 = W1.dot(X) + b1  # Linear transformation for the first layer\n    A1 = ReLU(Z1)        # Activation function for the first layer\n    Z2 = W2.dot(A1) + b2  # Linear transformation for the second layer\n    A2 = ReLU(Z2)        # Activation function for the second layer\n    Z3 = W3.dot(A2) + b3  # Linear transformation for the output layer\n    A3 = softmax(Z3)     # Applying softmax to get output probabilities\n    return Z1, A1, Z2, A2, Z3, A3\n\ndef ReLU_deriv(Z):\n  \"\"\"Calculates the derivative of the ReLU activation function.\"\"\"\n  return Z > 0\n\n\ndef one_hot(Y):\n    \"\"\"Converts the labels to one-hot encoding.\"\"\"\n    one_hot_Y = np.zeros((Y.size, Y.max() + 1))  # Initialize a zero array\n    one_hot_Y[np.arange(Y.size), Y] = 1           # Set the correct indices to 1\n    one_hot_Y = one_hot_Y.T                       # Transpose the array for correct shape\n    return one_hot_Y\n\n\ndef backward_prop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, Y):\n    \"\"\"Performs backward propagation to compute gradients.\"\"\"\n    one_hot_Y = one_hot(Y)  # Convert labels to one-hot encoding\n    dZ3 = A3 - one_hot_Y     # Compute the gradient for the output layer\n    dW3 = 1 / m * dZ3.dot(A2.T)  # Gradient for weights of output layer\n    db3 = 1 / m * np.sum(dZ3)     # Gradient for biases of output layer\n\n    # Backpropagate through the second layer\n    dZ2 = W3.T.dot(dZ3) * ReLU_deriv(Z2)\n    dW2 = 1 / m * dZ2.dot(A1.T)  # Gradient for weights of second layer\n    db2 = 1 / m * np.sum(dZ2)     # Gradient for biases of second layer\n\n    # Backpropagate through the first layer\n    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n    dW1 = 1 / m * dZ1.dot(X.T)    # Gradient for weights of first layer\n    db1 = 1 / m * np.sum(dZ1)      # Gradient for biases of first layer\n\n    return dW1, db1, dW2, db2, dW3, db3\n\ndef update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha):\n    \"\"\"Updates the parameters using gradient descent.\"\"\"\n    W1 = W1 - alpha * dW1  # Update weights of the first layer\n    b1 = b1 - alpha * db1   # Update biases of the first layer\n    W2 = W2 - alpha * dW2   # Update weights of the second layer\n    b2 = b2 - alpha * db2   # Update biases of the second layer\n    W3 = W3 - alpha * dW3   # Update weights of the output layer\n    b3 = b3 - alpha * db3   # Update biases of the output layer\n    return W1, b1, W2, b2, W3, b3","metadata":{"id":"n-Cn37-ZmbvT","execution":{"iopub.status.busy":"2024-10-23T18:06:12.184610Z","iopub.execute_input":"2024-10-23T18:06:12.185126Z","iopub.status.idle":"2024-10-23T18:06:12.209012Z","shell.execute_reply.started":"2024-10-23T18:06:12.185066Z","shell.execute_reply":"2024-10-23T18:06:12.207474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predictions(A3):\n    \"\"\"Returns the predicted classes based on the softmax output.\"\"\"\n    return np.argmax(A3, 0)  # Returns the index of the maximum probability for each sample\n\n\ndef get_accuracy(predictions, Y):\n    \"\"\"Calculates the accuracy of the model.\"\"\"\n    print(predictions, Y)  # Optional: Print predictions and true labels for inspection\n    return np.sum(predictions == Y) / Y.size  # Calculate the ratio of correct predictions\n\n\ndef gradient_descent(X, Y, alpha, iterations):\n    \"\"\"Trains the neural network using gradient descent.\"\"\"\n    W1, b1, W2, b2, W3, b3 = initialize_parameters()  # Initialize parameters\n    for i in range(1, iterations+1):\n        # Forward propagation\n        Z1, A1, Z2, A2, Z3, A3 = forward_prop(W1, b1, W2, b2, W3, b3, X)\n\n        # Backward propagation\n        dW1, db1, dW2, db2, dW3, db3 = backward_prop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, Y)\n\n        # Update parameters\n        W1, b1, W2, b2, W3, b3 = update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha)\n\n        # Print the progress every 50 iterations\n        if i % 50 == 0:\n            print(\"Iteration: \", i)\n            predictions = get_predictions(A3)\n            print(\"Accuracy: \", get_accuracy(predictions, Y))\n\n    return W1, b1, W2, b2, W3, b3","metadata":{"id":"-mZoLGQmoXBN","execution":{"iopub.status.busy":"2024-10-23T18:06:12.210539Z","iopub.execute_input":"2024-10-23T18:06:12.210963Z","iopub.status.idle":"2024-10-23T18:06:12.231417Z","shell.execute_reply.started":"2024-10-23T18:06:12.210922Z","shell.execute_reply":"2024-10-23T18:06:12.230137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the neural network on the training set\nW1, b1, W2, b2, W3, b3 = gradient_descent(X_train, Y_train, 0.1, 500)","metadata":{"id":"RYc2yBeEo7aS","outputId":"e21f79aa-879a-4962-c90b-715f4b1e335f","execution":{"iopub.status.busy":"2024-10-23T18:06:12.236032Z","iopub.execute_input":"2024-10-23T18:06:12.236579Z","iopub.status.idle":"2024-10-23T18:09:44.589721Z","shell.execute_reply.started":"2024-10-23T18:06:12.236516Z","shell.execute_reply":"2024-10-23T18:09:44.588102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_predictions(X, W1, b1, W2, b2, W3, b3):\n  \"\"\"Generates predictions for the input data using the trained network.\"\"\"\n  _, _, _, _, _, A3 = forward_prop(W1, b1, W2, b2, W3, b3, X)\n  predictions = get_predictions(A3)\n  return predictions\n\ndef test_prediction(index, W1, b1, W2, b2, W3, b3):\n    \"\"\"Tests the model's prediction for a specific image index.\"\"\"\n    current_image = X_train[:, index, None]  # Get the current image (column vector)\n    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2, W3, b3)  # Get prediction for the current image\n    label = Y_train[index]  # Get the true label for the current image\n    print(\"Prediction: \", prediction)  # Print the predicted class\n    print(\"Label: \", label)  # Print the true label\n\n    # Reshape the image for displaying and scale back to 0-255\n    current_image = current_image.reshape((28, 28)) * 255\n    plt.gray()  # Set the colormap to gray\n    plt.imshow(current_image, interpolation='nearest')  # Display the image\n    plt.show()  # Show the plot\n","metadata":{"id":"0hspLPyGo_qU","execution":{"iopub.status.busy":"2024-10-23T18:09:44.592072Z","iopub.execute_input":"2024-10-23T18:09:44.593075Z","iopub.status.idle":"2024-10-23T18:09:44.610428Z","shell.execute_reply.started":"2024-10-23T18:09:44.592988Z","shell.execute_reply":"2024-10-23T18:09:44.608796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indexes = [1,50,100,150,200,250,300,350,400,450]\nfor i in indexes:\n  test_prediction(i, W1, b1, W2, b2, W3, b3)","metadata":{"id":"ukyLYnj95Svt","outputId":"07cbf5d6-3c66-4cd4-81fa-f5cfeb31faf9","execution":{"iopub.status.busy":"2024-10-23T18:09:44.612958Z","iopub.execute_input":"2024-10-23T18:09:44.614018Z","iopub.status.idle":"2024-10-23T18:09:47.421177Z","shell.execute_reply.started":"2024-10-23T18:09:44.613928Z","shell.execute_reply":"2024-10-23T18:09:47.419792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing test data\ntest_data = pd.read_csv('/kaggle/input/mnist-dataset/mnist_test.csv')","metadata":{"id":"e2yq_TBXrjZa","execution":{"iopub.status.busy":"2024-10-23T18:09:47.422655Z","iopub.execute_input":"2024-10-23T18:09:47.423005Z","iopub.status.idle":"2024-10-23T18:09:48.295086Z","shell.execute_reply.started":"2024-10-23T18:09:47.422967Z","shell.execute_reply":"2024-10-23T18:09:48.293798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting the test data to a NumPy array\ntest_data = np.array(test_data)","metadata":{"id":"iozqfPbLrobS","execution":{"iopub.status.busy":"2024-10-23T18:09:48.297021Z","iopub.execute_input":"2024-10-23T18:09:48.297532Z","iopub.status.idle":"2024-10-23T18:09:48.335181Z","shell.execute_reply.started":"2024-10-23T18:09:48.297474Z","shell.execute_reply":"2024-10-23T18:09:48.333945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the number of samples (m_test) and features (n_test) from the test dataset\nm_test, n_test = test_data.shape","metadata":{"id":"pO1ib8NGrrLb","execution":{"iopub.status.busy":"2024-10-23T18:09:48.336733Z","iopub.execute_input":"2024-10-23T18:09:48.337099Z","iopub.status.idle":"2024-10-23T18:09:48.342781Z","shell.execute_reply.started":"2024-10-23T18:09:48.337062Z","shell.execute_reply":"2024-10-23T18:09:48.341471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preparing the test data\ntest_data_dev = test_data.T  # Transposing the test data for easier indexing\nY_test = test_data_dev[0]    # Labels for the test set\nX_test = test_data_dev[1:n_test]  # Features for the test set\nX_test = X_test / 255         # Normalizing the test input data (values between 0 and 1)\n","metadata":{"id":"o7IJiiiyryHc","execution":{"iopub.status.busy":"2024-10-23T18:09:48.344307Z","iopub.execute_input":"2024-10-23T18:09:48.344745Z","iopub.status.idle":"2024-10-23T18:09:48.369259Z","shell.execute_reply.started":"2024-10-23T18:09:48.344704Z","shell.execute_reply":"2024-10-23T18:09:48.368109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making predictions on the test set and calculating accuracy\ntest_data_predictions = make_predictions(X_test, W1, b1, W2, b2, W3, b3)  # Get predictions for the test set\ntest_accuracy = get_accuracy(test_data_predictions, Y_test)  # Calculate accuracy for the test set\nprint(\"Test Set Accuracy: \", test_accuracy)  # Print the accuracy","metadata":{"id":"PL7ASurOsBk8","outputId":"f3b9200f-c756-49b1-d21b-51417b73f61f","execution":{"iopub.status.busy":"2024-10-23T18:09:48.370883Z","iopub.execute_input":"2024-10-23T18:09:48.371397Z","iopub.status.idle":"2024-10-23T18:09:48.426687Z","shell.execute_reply.started":"2024-10-23T18:09:48.371338Z","shell.execute_reply":"2024-10-23T18:09:48.425423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"DJs-CGxTsKJr"},"execution_count":null,"outputs":[]}]}